{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xorqpTpAV18P",
        "nBs6pDsiSd-Y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "eOT0_ldzfkBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d482525d-9bc2-4e0c-bf52-b5b5fe25f071"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "TSEh-iK-I7d7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j-OVhbBGfISI"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "from torch.types import Number\n",
        "import sys\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constant Variables"
      ],
      "metadata": {
        "id": "6bVWAUIxVojW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROUND_PATH = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/DataSet/testA\"\n",
        "SEGMENT_PATH = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/DataSet/testB\"\n",
        "\n",
        "GROUND_PATH_TEST = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/DataSet/testA\"\n",
        "SEGMENT_PATH_TEST = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/DataSet/testB\"\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-5\n",
        "LAMBDA_IDENTITY = 0.0\n",
        "LAMBDA_CYCLE = 10\n",
        "WEIGHTS_GEN_G = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/GEN_G\"\n",
        "WEIGHTS_GEN_S = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/GEN_S\"\n",
        "WEIGHTS_DISC_G = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/DISC_G\"\n",
        "WEIGHTS_DISC_S = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/DISC_S\"\n",
        "SAVE_IMAGE = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Images\"\n",
        "\n",
        "\n",
        "transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=256, height=256),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
        "        ToTensor(),\n",
        "     ],\n",
        "    additional_targets={\"image\": \"image\",\n",
        "                        'image1': 'image',},\n",
        ")\n",
        "\n",
        "transforms2 = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=256, height=256),\n",
        "        ToTensor(),\n",
        "     ],\n",
        ")\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = 4\n",
        "NUM_EPOCHS = 1000"
      ],
      "metadata": {
        "id": "lKEvvEdRVn3V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 1000"
      ],
      "metadata": {
        "id": "gUbpBMgUk5UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "7XlppgzSw4uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    # optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # for param_group in optimizer.param_groups:\n",
        "    #     param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "_MDUWpbIw4UW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataSet"
      ],
      "metadata": {
        "id": "xorqpTpAV18P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CityscapesDataSet(Dataset):\n",
        "    def __init__(self, SegmentationPath, GroundImagePath, transform=None):\n",
        "        self.GroundImagePath = GroundImagePath\n",
        "        self.SegmentationPath = SegmentationPath\n",
        "        self.transform = transform\n",
        "\n",
        "        self.GroundImages = os.listdir(GroundImagePath)\n",
        "        self.SegmentImages = os.listdir(SegmentationPath)\n",
        "        self.length_dataset = max(len(self.GroundImages), len(self.SegmentImages)) \n",
        "        self.Gr_len = len(self.GroundImages)\n",
        "        self.Sg_len = len(self.SegmentImages)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length_dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        Ground = self.GroundImages[index % self.Gr_len]\n",
        "        Segment = self.SegmentImages[index % self.Sg_len]\n",
        "\n",
        "\n",
        "        Ground = np.array(Image.open(os.path.join(self.GroundImagePath, Ground)).convert(\"RGB\"))\n",
        "        Segment = np.array(Image.open(os.path.join(self.SegmentationPath, Segment)).convert(\"RGB\"))\n",
        "\n",
        "        if self.transform:\n",
        "            Ground = self.transform(image=Ground)\n",
        "            Segment = self.transform(image=Segment)\n",
        "            Ground = Ground['image']\n",
        "            Segment = Segment['image']\n",
        "\n",
        "        return Segment, Ground"
      ],
      "metadata": {
        "id": "TNYKRpXTV6GP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator"
      ],
      "metadata": {
        "id": "YqZAcAJJSXEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding, down=True, use_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        if down:\n",
        "          self.conv1 = nn.Conv2d(in_channel, out_channel, padding_mode=\"reflect\", kernel_size=kernel_size, padding=padding, **kwargs)\n",
        "        else:\n",
        "          self.conv1 = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, **kwargs)\n",
        "        self.IN = nn.InstanceNorm2d(out_channel)\n",
        "        if use_act:\n",
        "          self.acv = nn.ReLU(inplace=True)\n",
        "        else:\n",
        "          self.acv = nn.Identity()\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.IN(x)\n",
        "      return  self.acv(x)"
      ],
      "metadata": {
        "id": "KMhkBIsOSWZx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.Conv1 = ConvBlock(channels, channels, kernel_size=3, padding=1)\n",
        "    self.Conv2 = ConvBlock(channels, channels, kernel_size=3, padding=1, use_act=False)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x2 = self.Conv1(x)\n",
        "    x2 = self.Conv2(x2)\n",
        "    return x + x2"
      ],
      "metadata": {
        "id": "u6cqvJqZuvW1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, img_channels, num_features=64, num_residual=9):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "    self.acv1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.DownBlock = nn.ModuleList([\n",
        "        nn.Conv2d(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
        "        nn.Conv2d(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
        "    ])\n",
        "\n",
        "    self.residual_blocks = nn.Sequential(\n",
        "        *[ResBlock(num_features*4) for _ in range(num_residual)]\n",
        "    )\n",
        "\n",
        "    self.up_blocks = nn.ModuleList([\n",
        "        ConvBlock(num_features*4, num_features*2, kernel_size=3, stride=2, padding=1, output_padding=1, down=False),\n",
        "        ConvBlock(num_features*2, num_features*1, kernel_size=3, stride=2, padding=1, output_padding=1, down=False),\n",
        "    ])\n",
        "\n",
        "    self.last_layer = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.acv1(x)\n",
        "    for layer in self.DownBlock:\n",
        "      x = layer(x)\n",
        "\n",
        "    for layer in self.up_blocks:\n",
        "      x = layer(x)\n",
        "    return torch.tanh(self.last_layer(x))\n",
        "    \n"
      ],
      "metadata": {
        "id": "bd4faoHsux50"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "nBs6pDsiSd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, stride):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channel, out_channel, 4, stride, 1, bias=True, padding_mode=\"reflect\")\n",
        "    self.IN1 = nn.InstanceNorm2d(out_channel)\n",
        "    self.act1 = nn.LeakyReLU(0.2)\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.IN1(x)\n",
        "    x = self.act1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RydVOs8Cgcmm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channel, num_features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channel, num_features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\")\n",
        "    self.act1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    layers = []\n",
        "    in_channel = num_features[0]\n",
        "    \n",
        "    for counter in range(1,len(num_features)):\n",
        "\n",
        "      layers.append(DiscBlock(in_channel, num_features[counter], stride=1 if counter==(len(num_features)-1) else 2))\n",
        "      in_channel = num_features[counter]\n",
        "    layers.append(nn.Conv2d(in_channel, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.model(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "M8T-FBTESrPw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Y_tLfkrbvZp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler):\n",
        "  H_reals = 0\n",
        "  H_fakes = 0\n",
        "\n",
        "  loop_prop = tqdm(loader, leave=True)\n",
        "  index2 = 0\n",
        "  for index, (segm, grd) in enumerate(loop_prop):\n",
        "    grd = grd.to(DEVICE)\n",
        "    segm = segm.to(DEVICE)\n",
        "    \n",
        "    \n",
        "    with torch.cuda.amp.autocast():\n",
        "        fake_ground = Gen_Ground(segm)\n",
        "        D_H_real = Disc_Ground(grd)\n",
        "        D_H_fake = Disc_Ground(fake_ground.detach())\n",
        "        H_reals += D_H_real.mean().item()\n",
        "        H_fakes += D_H_fake.mean().item()\n",
        "        D_H_real_loss = MSE(D_H_real, torch.ones_like(D_H_real))\n",
        "        D_H_fake_loss = MSE(D_H_fake, torch.zeros_like(D_H_fake))\n",
        "        D_H_loss = D_H_real_loss + D_H_fake_loss\n",
        "       \n",
        "        fake_seg = Gen_Seg(grd)\n",
        "        D_Z_real = Disc_Seg(segm)\n",
        "        D_Z_fake = Disc_Seg(fake_seg.detach())\n",
        "        D_Z_real_loss = MSE(D_Z_real, torch.ones_like(D_Z_real))\n",
        "        D_Z_fake_loss = MSE(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
        "        D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n",
        "\n",
        "        \n",
        "        D_loss = (D_H_loss + D_Z_loss)/2\n",
        "\n",
        "    optim_Disc.zero_grad()\n",
        "    Disc_scaler.scale(D_loss).backward()\n",
        "    Disc_scaler.step(optim_Disc)\n",
        "    Disc_scaler.update()\n",
        "\n",
        "    # Train Generators H and Z\n",
        "    with torch.cuda.amp.autocast():\n",
        "        # adversarial loss for both generators\n",
        "        D_H_fake = Disc_Ground(fake_ground)\n",
        "        D_Z_fake = Disc_Seg(fake_seg)\n",
        "        loss_G_H = MSE(D_H_fake, torch.ones_like(D_H_fake))\n",
        "        loss_G_Z = MSE(D_Z_fake, torch.ones_like(D_Z_fake))\n",
        "\n",
        "        # cycle loss\n",
        "        cycle_segm = Gen_Seg(fake_ground)\n",
        "        cycle_ground = Gen_Ground(fake_seg)\n",
        "        cycle_segm_loss = L1(segm, cycle_segm)\n",
        "        cycle_ground_loss = L1(grd, cycle_ground)\n",
        "\n",
        "        # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
        "        identity_segm = Gen_Seg(segm)\n",
        "        identity_ground = Gen_Ground(grd)\n",
        "        identity_segm_loss = L1(segm, identity_segm)\n",
        "        identity_ground_loss = L1(grd, identity_ground)\n",
        "\n",
        "        # add all togethor\n",
        "        G_loss = (\n",
        "            loss_G_Z\n",
        "            + loss_G_H\n",
        "            + cycle_segm_loss * LAMBDA_CYCLE\n",
        "            + cycle_ground_loss * LAMBDA_CYCLE\n",
        "            + identity_ground_loss * LAMBDA_IDENTITY\n",
        "            + identity_segm_loss * LAMBDA_IDENTITY\n",
        "        )\n",
        "\n",
        "    optim_Gen.zero_grad()\n",
        "    Gen_scaler.scale(G_loss).backward()\n",
        "    Gen_scaler.step(optim_Gen)\n",
        "    Gen_scaler.update()\n",
        "\n",
        "    if index2 % 200 == 0:\n",
        "        save_image(fake_ground, SAVE_IMAGE + f\"/Ground/{index2}.png\")\n",
        "        save_image(fake_seg, SAVE_IMAGE + f\"/Segment/{index2}.png\")\n",
        "    index2 += 1\n",
        "    loop_prop.set_postfix(H_real=H_reals/(index+1), H_fake=H_fakes/(index+1))\n",
        "\n"
      ],
      "metadata": {
        "id": "EpIT6DOmcjQY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "DjLn1BDan328"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LOAD_MODEL=True\n",
        "SAVE_MODEL=True\n",
        "\n",
        "\n",
        "Disc_Ground = Discriminator(in_channel=3).to(DEVICE)\n",
        "Disc_Seg = Discriminator(in_channel=3).to(DEVICE)\n",
        "Gen_Ground = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "Gen_Seg = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optim_Disc = optim.Adam(list(Disc_Ground.parameters()) + list(Disc_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "optim_Gen = optim.Adam(list(Disc_Ground.parameters()) + list(Gen_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "MSE = nn.MSELoss()\n",
        "if LOAD_MODEL:\n",
        "\n",
        "  Disc_Ground.load_state_dict(torch.load(WEIGHTS_DISC_G))\n",
        "  Disc_Seg.load_state_dict(torch.load(WEIGHTS_DISC_S))\n",
        "  Gen_Ground.load_state_dict(torch.load(WEIGHTS_GEN_G))\n",
        "  Gen_Seg.load_state_dict(torch.load(WEIGHTS_GEN_S))\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_G, Gen_Ground, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_S, Gen_Seg, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_G, Disc_Ground, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_S, Disc_Seg, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "dataset = CityscapesDataSet(SEGMENT_PATH, GROUND_PATH, transforms2)\n",
        "test_dataset = CityscapesDataSet(SEGMENT_PATH_TEST, GROUND_PATH_TEST, transforms2)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size=1,\n",
        "  shuffle=False,\n",
        "  pin_memory=True,\n",
        ")\n",
        "\n",
        "Gen_scaler = torch.cuda.amp.GradScaler()\n",
        "Disc_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler)\n",
        "\n",
        "\n",
        "    if SAVE_MODEL:\n",
        "      torch.save(Disc_Ground.state_dict(), WEIGHTS_DISC_G)\n",
        "      torch.save(Disc_Seg.state_dict(), WEIGHTS_DISC_S)\n",
        "\n",
        "      torch.save(Gen_Ground.state_dict(), WEIGHTS_GEN_G)\n",
        "      torch.save(Gen_Seg.state_dict(), WEIGHTS_GEN_S)\n",
        "\n"
      ],
      "metadata": {
        "id": "z66W2LNhvZje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d65fa77-e5ad-4303-a668-e0d6895afa30"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 500/500 [00:52<00:00,  9.57it/s, H_fake=0.532, H_real=0.617]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.548, H_real=0.64]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.557, H_real=0.65]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.565, H_real=0.659]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.571, H_real=0.667]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.577, H_real=0.674]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.03it/s, H_fake=0.583, H_real=0.682]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.05it/s, H_fake=0.588, H_real=0.69]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.592, H_real=0.697]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.596, H_real=0.705]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.6, H_real=0.709]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.20it/s, H_fake=0.606, H_real=0.714]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.20it/s, H_fake=0.61, H_real=0.716]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.615, H_real=0.72]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.619, H_real=0.724]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.623, H_real=0.729]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.627, H_real=0.735]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.26it/s, H_fake=0.63, H_real=0.741]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.634, H_real=0.745]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.638, H_real=0.752]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.64, H_real=0.756]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.645, H_real=0.759]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.648, H_real=0.763]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.652, H_real=0.764]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.656, H_real=0.769]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.659, H_real=0.772]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.663, H_real=0.778]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.665, H_real=0.783]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.668, H_real=0.785]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.671, H_real=0.791]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.674, H_real=0.792]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.676, H_real=0.796]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.678, H_real=0.798]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.68, H_real=0.802]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.683, H_real=0.805]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.685, H_real=0.807]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.686, H_real=0.809]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.689, H_real=0.812]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.69, H_real=0.813]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.693, H_real=0.814]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.696, H_real=0.815]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.697, H_real=0.815]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.7, H_real=0.815]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.702, H_real=0.815]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.703, H_real=0.817]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.705, H_real=0.819]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.706, H_real=0.819]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.18it/s, H_fake=0.708, H_real=0.82]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.709, H_real=0.822]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.71, H_real=0.823]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.712, H_real=0.824]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.714, H_real=0.823]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.715, H_real=0.824]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.716, H_real=0.826]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.717, H_real=0.827]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.718, H_real=0.829]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.719, H_real=0.829]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.721, H_real=0.829]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.722, H_real=0.83]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.723, H_real=0.83]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.723, H_real=0.832]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.724, H_real=0.833]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.726, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.727, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.05it/s, H_fake=0.728, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.729, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.73, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.731, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.732, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.733, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.06it/s, H_fake=0.734, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.735, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.736, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.737, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.738, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.739, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.74, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.741, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.742, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.743, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.04it/s, H_fake=0.743, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.01it/s, H_fake=0.745, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.746, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.747, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.747, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.09it/s, H_fake=0.748, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.749, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.75, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.751, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.752, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.753, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.753, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.754, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.755, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.755, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.756, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.756, H_real=0.838]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.757, H_real=0.838]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.758, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.758, H_real=0.838]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_GEN_G = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/GEN_G\"\n",
        "WEIGHTS_GEN_S = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/GEN_S\"\n",
        "WEIGHTS_DISC_G = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/DISC_G\"\n",
        "WEIGHTS_DISC_S = \"/content/drive/MyDrive/Courses/York Courses/Neural Networks and Deep Learning/Project/Weights/DISC_S\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JWo-8siZOIXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LOAD_MODEL=True\n",
        "SAVE_MODEL=True\n",
        "\n",
        "\n",
        "Disc_Ground = Discriminator(in_channel=3).to(DEVICE)\n",
        "Disc_Seg = Discriminator(in_channel=3).to(DEVICE)\n",
        "Gen_Ground = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "Gen_Seg = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optim_Disc = optim.Adam(list(Disc_Ground.parameters()) + list(Disc_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "optim_Gen = optim.Adam(list(Disc_Ground.parameters()) + list(Gen_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "MSE = nn.MSELoss()\n",
        "if LOAD_MODEL:\n",
        "\n",
        "  Disc_Ground.load_state_dict(torch.load(WEIGHTS_DISC_G))\n",
        "  Disc_Seg.load_state_dict(torch.load(WEIGHTS_DISC_S))\n",
        "  Gen_Ground.load_state_dict(torch.load(WEIGHTS_GEN_G))\n",
        "  Gen_Seg.load_state_dict(torch.load(WEIGHTS_GEN_S))\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_G, Gen_Ground, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_S, Gen_Seg, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_G, Disc_Ground, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_S, Disc_Seg, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "dataset = CityscapesDataSet(SEGMENT_PATH, GROUND_PATH, transforms2)\n",
        "test_dataset = CityscapesDataSet(SEGMENT_PATH_TEST, GROUND_PATH_TEST, transforms2)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size=1,\n",
        "  shuffle=False,\n",
        "  pin_memory=True,\n",
        ")\n",
        "\n",
        "Gen_scaler = torch.cuda.amp.GradScaler()\n",
        "Disc_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler)\n",
        "\n",
        "\n",
        "    if SAVE_MODEL:\n",
        "      torch.save(Disc_Ground.state_dict(), WEIGHTS_DISC_G)\n",
        "      torch.save(Disc_Seg.state_dict(), WEIGHTS_DISC_S)\n",
        "\n",
        "      torch.save(Gen_Ground.state_dict(), WEIGHTS_GEN_G)\n",
        "      torch.save(Gen_Seg.state_dict(), WEIGHTS_GEN_S)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NSgm441HNWwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839c5a72-ad3f-42c6-8b49-e64bd6b7c5f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.752, H_real=0.831]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.754, H_real=0.832]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.755, H_real=0.833]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.756, H_real=0.833]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.756, H_real=0.833]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.757, H_real=0.833]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.757, H_real=0.833]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.758, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.759, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.759, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.76, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.761, H_real=0.834]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.762, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.762, H_real=0.835]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.762, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.762, H_real=0.836]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.763, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.763, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.764, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.764, H_real=0.837]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.765, H_real=0.838]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.765, H_real=0.838]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.765, H_real=0.838]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.766, H_real=0.838]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.767, H_real=0.839]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.767, H_real=0.839]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.767, H_real=0.839]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.768, H_real=0.839]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.37it/s, H_fake=0.769, H_real=0.84]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.768, H_real=0.839]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.769, H_real=0.84]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.769, H_real=0.84]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.77, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.20it/s, H_fake=0.77, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.77, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.77, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.771, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.771, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.771, H_real=0.841]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.772, H_real=0.842]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.773, H_real=0.842]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.773, H_real=0.842]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.773, H_real=0.843]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.06it/s, H_fake=0.773, H_real=0.842]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.774, H_real=0.843]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.774, H_real=0.843]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.01it/s, H_fake=0.774, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.775, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.776, H_real=0.843]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.03it/s, H_fake=0.775, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.09it/s, H_fake=0.775, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.776, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.777, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.777, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.777, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.777, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.778, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.778, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.778, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.778, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.779, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.779, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.779, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.779, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.09it/s, H_fake=0.78, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.78, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.78, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.78, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.781, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.781, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.781, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.781, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.782, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.20it/s, H_fake=0.782, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.782, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.782, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.782, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.18it/s, H_fake=0.782, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.783, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.20it/s, H_fake=0.783, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.784, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.783, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.784, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.784, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.785, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.785, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.785, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.785, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.26it/s, H_fake=0.786, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.786, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.786, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.786, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.787, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.787, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.787, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.787, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.788, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.36it/s, H_fake=0.788, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.788, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.788, H_real=0.851]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LOAD_MODEL=True\n",
        "SAVE_MODEL=True\n",
        "\n",
        "\n",
        "Disc_Ground = Discriminator(in_channel=3).to(DEVICE)\n",
        "Disc_Seg = Discriminator(in_channel=3).to(DEVICE)\n",
        "Gen_Ground = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "Gen_Seg = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optim_Disc = optim.Adam(list(Disc_Ground.parameters()) + list(Disc_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "optim_Gen = optim.Adam(list(Disc_Ground.parameters()) + list(Gen_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "MSE = nn.MSELoss()\n",
        "if LOAD_MODEL:\n",
        "\n",
        "  Disc_Ground.load_state_dict(torch.load(WEIGHTS_DISC_G))\n",
        "  Disc_Seg.load_state_dict(torch.load(WEIGHTS_DISC_S))\n",
        "  Gen_Ground.load_state_dict(torch.load(WEIGHTS_GEN_G))\n",
        "  Gen_Seg.load_state_dict(torch.load(WEIGHTS_GEN_S))\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_G, Gen_Ground, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_S, Gen_Seg, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_G, Disc_Ground, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_S, Disc_Seg, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "dataset = CityscapesDataSet(SEGMENT_PATH, GROUND_PATH, transforms2)\n",
        "test_dataset = CityscapesDataSet(SEGMENT_PATH_TEST, GROUND_PATH_TEST, transforms2)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size=1,\n",
        "  shuffle=False,\n",
        "  pin_memory=True,\n",
        ")\n",
        "\n",
        "Gen_scaler = torch.cuda.amp.GradScaler()\n",
        "Disc_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler)\n",
        "\n",
        "\n",
        "    if SAVE_MODEL:\n",
        "      torch.save(Disc_Ground.state_dict(), WEIGHTS_DISC_G)\n",
        "      torch.save(Disc_Seg.state_dict(), WEIGHTS_DISC_S)\n",
        "\n",
        "      torch.save(Gen_Ground.state_dict(), WEIGHTS_GEN_G)\n",
        "      torch.save(Gen_Seg.state_dict(), WEIGHTS_GEN_S)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5xZXq387Mie",
        "outputId": "036c2daa-bf6a-4c6f-d2c4-d420f858d09d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.783, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.783, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.784, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.784, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.785, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.785, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.785, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.785, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.36it/s, H_fake=0.785, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.37it/s, H_fake=0.786, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.36it/s, H_fake=0.786, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.37it/s, H_fake=0.786, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.38it/s, H_fake=0.787, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.39it/s, H_fake=0.787, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.787, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.787, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.36it/s, H_fake=0.788, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.788, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.21it/s, H_fake=0.788, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.788, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.37it/s, H_fake=0.789, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.34it/s, H_fake=0.788, H_real=0.848]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.789, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.36it/s, H_fake=0.789, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.36it/s, H_fake=0.789, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.789, H_real=0.849]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.789, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.79, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.789, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.79, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.79, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:55<00:00,  8.99it/s, H_fake=0.79, H_real=0.85]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.03it/s, H_fake=0.79, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.79, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.79, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.791, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.791, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.791, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.05it/s, H_fake=0.791, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.791, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.18it/s, H_fake=0.791, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.792, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.792, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.792, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.08it/s, H_fake=0.793, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.792, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.793, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.793, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.793, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.18it/s, H_fake=0.793, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.794, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.794, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.794, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.794, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.20it/s, H_fake=0.794, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.795, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.795, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.795, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.796, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.796, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.796, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.796, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.797, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.01it/s, H_fake=0.797, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.06it/s, H_fake=0.797, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.03it/s, H_fake=0.797, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.03it/s, H_fake=0.797, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.798, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.797, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.798, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.798, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.23it/s, H_fake=0.798, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.798, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.798, H_real=0.851]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.799, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.35it/s, H_fake=0.799, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.31it/s, H_fake=0.799, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.26it/s, H_fake=0.799, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.8, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.8, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.8, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.8, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.8, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.8, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.33it/s, H_fake=0.801, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.29it/s, H_fake=0.801, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.19it/s, H_fake=0.801, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.18it/s, H_fake=0.801, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.801, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.24it/s, H_fake=0.802, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.802, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.802, H_real=0.852]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.27it/s, H_fake=0.802, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.802, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.802, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.803, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.802, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.28it/s, H_fake=0.803, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.25it/s, H_fake=0.802, H_real=0.853]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.803, H_real=0.854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LOAD_MODEL=True\n",
        "SAVE_MODEL=True\n",
        "\n",
        "\n",
        "Disc_Ground = Discriminator(in_channel=3).to(DEVICE)\n",
        "Disc_Seg = Discriminator(in_channel=3).to(DEVICE)\n",
        "Gen_Ground = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "Gen_Seg = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optim_Disc = optim.Adam(list(Disc_Ground.parameters()) + list(Disc_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "optim_Gen = optim.Adam(list(Disc_Ground.parameters()) + list(Gen_Seg.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "MSE = nn.MSELoss()\n",
        "if LOAD_MODEL:\n",
        "\n",
        "  Disc_Ground.load_state_dict(torch.load(WEIGHTS_DISC_G))\n",
        "  Disc_Seg.load_state_dict(torch.load(WEIGHTS_DISC_S))\n",
        "  Gen_Ground.load_state_dict(torch.load(WEIGHTS_GEN_G))\n",
        "  Gen_Seg.load_state_dict(torch.load(WEIGHTS_GEN_S))\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_G, Gen_Ground, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_GEN_S, Gen_Seg, optim_Gen, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_G, Disc_Ground, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "    # load_checkpoint(\n",
        "    #     WEIGHTS_DISC_S, Disc_Seg, optim_Disc, LEARNING_RATE,\n",
        "    # )\n",
        "dataset = CityscapesDataSet(SEGMENT_PATH, GROUND_PATH, transforms2)\n",
        "test_dataset = CityscapesDataSet(SEGMENT_PATH_TEST, GROUND_PATH_TEST, transforms2)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size=1,\n",
        "  shuffle=False,\n",
        "  pin_memory=True,\n",
        ")\n",
        "\n",
        "Gen_scaler = torch.cuda.amp.GradScaler()\n",
        "Disc_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler)\n",
        "\n",
        "\n",
        "    if SAVE_MODEL:\n",
        "      torch.save(Disc_Ground.state_dict(), WEIGHTS_DISC_G)\n",
        "      torch.save(Disc_Seg.state_dict(), WEIGHTS_DISC_S)\n",
        "\n",
        "      torch.save(Gen_Ground.state_dict(), WEIGHTS_GEN_G)\n",
        "      torch.save(Gen_Seg.state_dict(), WEIGHTS_GEN_S)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZX4OYF3foqi",
        "outputId": "33f14984-07ad-4511-b9f3-7c2df918f872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.32it/s, H_fake=0.795, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:53<00:00,  9.30it/s, H_fake=0.795, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.795, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.22it/s, H_fake=0.796, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.796, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.796, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.797, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.797, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.797, H_real=0.844]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.08it/s, H_fake=0.797, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.797, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.07it/s, H_fake=0.797, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.09it/s, H_fake=0.797, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.798, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.797, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.17it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.798, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.06it/s, H_fake=0.799, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.799, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:56<00:00,  8.90it/s, H_fake=0.799, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.799, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.799, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.799, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.8, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.8, H_real=0.847]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.8, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.08it/s, H_fake=0.8, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.801, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.09it/s, H_fake=0.801, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.06it/s, H_fake=0.801, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.14it/s, H_fake=0.801, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.801, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.03it/s, H_fake=0.801, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.802, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.801, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.802, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.802, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.16it/s, H_fake=0.802, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.802, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.15it/s, H_fake=0.803, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.13it/s, H_fake=0.803, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.803, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.08it/s, H_fake=0.803, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.803, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.11it/s, H_fake=0.803, H_real=0.846]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.10it/s, H_fake=0.803, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.04it/s, H_fake=0.803, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:54<00:00,  9.12it/s, H_fake=0.804, H_real=0.845]\n",
            "100%|██████████| 500/500 [00:55<00:00,  9.09it/s, H_fake=0.804, H_real=0.845]\n",
            " 18%|█▊        | 89/500 [00:10<00:44,  9.15it/s, H_fake=0.804, H_real=0.845]"
          ]
        }
      ]
    }
  ]
}